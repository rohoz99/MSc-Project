{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom gensim.models.doc2vec import TaggedDocument, Doc2Vec\nfrom gensim.models.word2vec import Word2Vec\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import neighbors\nimport pickle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport nltk\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n\n\nfrom nltk.corpus import stopwords\nimport multiprocessing\n\ncores = multiprocessing.cpu_count()\ndoc2VecModel = \"doc2Vec_model.sav\"\nLRdoc2VecModel = \"LR_doc2Vec_model.sav\"\n\npath1 = \"../input/ps-dataset/powershell_benign_dataset/\"\npath2 = \"../input/ps-dataset/malicious_pure\"\n#path3 = r\"D:\\Test Scripts\"\nlabels = []\ntext = []\ntestText = []\nnewText = []\n#new_str = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', string)\nfName= []\nregex = r':\\s*\\n+'\nvectorizer = CountVectorizer(stop_words='english', max_features=5000)\n\ndef test_bowser(path):\n    for filename in os.listdir(path):\n        fName.append(filename)\n        filename = os.path.join(path, filename)\n        with open(filename, encoding='utf-8') as f:\n            t_content = f.read()\n            t_content = t_content.lower()\n            t_content = t_content.replace(\"(\", \"\")\n            t_content = t_content.replace(\")\", \"\")\n            t_content = t_content.replace(\"[\", \"\")\n            t_content = t_content.replace(\"]\", \"\")\n            t_content = t_content.replace(\"$\", \"\")\n            t_content = t_content.replace(\"-\", \"\")\n            t_content = t_content.replace(\"=\", \"\")\n            t_content = t_content.replace(\"\", \"\")\n            t_content = t_content.replace(\"{\", \"\")\n            t_content = t_content.replace(\"}\", \"\")\n            t_content = t_content.replace(\".\", \"\")\n            t_content = t_content.replace(\"_\", \"\")\n            t_content = t_content.replace(\"|\", \"\")\n            t_content = t_content.replace(\":\", \"\")\n            t_content = t_content.replace(\"|\", \"\")\n            t_content = t_content.replace(\"/\", \"\")\n            t_content = t_content.replace(\"@\", \"\")\n            t_content = t_content.replace(\"?\", \"\")\n            t_content = t_content.replace(\",\", \"\")\n            t_content = t_content.replace(\">\", \"\")\n            t_content = t_content.replace(\"<\", \"\")\n            t_content = t_content.replace(\";\", \"\")\n            t_content = t_content.replace(\"\\\\\", \"\")\n            t_content = t_content.replace(\"+\", \"\")\n            t_content = t_content.replace(\"\\\"\",\"\")\n            t_content = t_content.replace(\"\\'\",\"\")\n            t_content = t_content.replace(\"`\", \"\")\n            t_content = t_content.replace(\"*\", \"\")\n            t_content = \" \".join(t_content.split())\n            newText.append(t_content)\n\ndef bowser(path, label):\n    for filename in os.listdir(path):\n        labels.append(label)\n        filename = os.path.join(path, filename)\n        with open(filename, encoding='utf-8') as f:\n                    content = f.read()\n                    content = content.lower()\n                    content = content.replace(\"(\", \"\")\n                    content = content.replace(\")\", \"\")\n                    content = content.replace(\"[\", \"\")\n                    content = content.replace(\"]\", \"\")\n                    content = content.replace(\"$\", \"\")\n                    content = content.replace(\"-\", \"\")\n                    content = content.replace(\"=\", \"\")\n                    content = content.replace(\"\", \"\")\n                    content = content.replace(\"{\", \"\")\n                    content = content.replace(\"}\", \"\")\n                    content = content.replace(\".\", \"\")\n                    content = content.replace(\"_\", \"\")\n                    content = content.replace(\"|\", \"\")\n                    content = content.replace(\":\", \"\")\n                    content = content.replace(\"|\", \"\")\n                    content = content.replace(\"/\", \"\")\n                    content = content.replace(\"@\", \"\")\n                    content = content.replace(\"?\", \"\")\n                    content = content.replace(\",\", \"\")\n                    content = content.replace(\">\", \"\")\n                    content = content.replace(\"<\", \"\")\n                    content = content.replace(\";\", \"\")\n                    content = content.replace(\"\\\\\", \"\")\n                    content = content.replace(\"+\", \"\")\n                    content = content.replace(\"\\\"\", \"\")\n                    content = content.replace(\"\\'\", \"\")\n                    content = content.replace(\"`\", \"\")\n                    content = content.replace(\"*\", \"\")\n                    content = \" \".join(content.split())\n                    testText.append(content)\n\n\n#test_bowser(path3)\nbowser(path1,\"Benign\")\nbowser(path2,\"Malicious\")\n#print(testText[6])\nnumFiles = len(testText)\n\ndf = pd.DataFrame(testText, index=range(numFiles), columns=[\"Powershell Code\"])\ndf['Labels'] = labels\n#df.to_csv('tester.csv')\nprint(df)\n\ntrain, test = train_test_split(df, test_size=0.3, random_state=42)\n\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            if len(word) < 2:\n                continue\n            tokens.append(word.lower())\n    return tokens\n\ntrain_tagged = train.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Powershell Code']), tags=[r.Labels]), axis=1)\ntest_tagged = test.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Powershell Code']), tags=[r.Labels]), axis=1)\n\nmodel_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\nmodel_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n\ndef vec_for_learning(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n    return targets, regressors\n\n#Saving doc2vec model\npickle.dump(model_dbow, open(doc2VecModel, 'wb'))\n\n\ny_train, X_train = vec_for_learning(model_dbow, train_tagged)\ny_test, X_test = vec_for_learning(model_dbow, test_tagged)\n\n# logreg = LogisticRegression(n_jobs=1, C=1e5)\n# logreg.fit(X_train, y_train)\n\n# #Saving LR model\n# pickle.dump(logreg, open(LRdoc2VecModel, 'wb'))\n\n# y_pred = logreg.predict(X_test)\n\n# print(classification_report(y_test,y_pred))\n\n# #print(classification)\n# #print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n# #print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-02T19:42:59.103139Z","iopub.execute_input":"2022-09-02T19:42:59.103534Z","iopub.status.idle":"2022-09-02T19:44:28.261338Z","shell.execute_reply.started":"2022-09-02T19:42:59.103499Z","shell.execute_reply":"2022-09-02T19:44:28.260079Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:45:50.359048Z","iopub.execute_input":"2022-09-02T19:45:50.359422Z","iopub.status.idle":"2022-09-02T19:45:50.365047Z","shell.execute_reply.started":"2022-09-02T19:45:50.359391Z","shell.execute_reply":"2022-09-02T19:45:50.364134Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for x in range train['Powershell Code']:\n    words = tokenize_text(str(train['Powershell Code']))","metadata":{"execution":{"iopub.status.busy":"2022-09-02T20:11:24.678537Z","iopub.execute_input":"2022-09-02T20:11:24.678949Z","iopub.status.idle":"2022-09-02T20:11:24.686890Z","shell.execute_reply.started":"2022-09-02T20:11:24.678914Z","shell.execute_reply":"2022-09-02T20:11:24.685014Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = train['Powershell Code']\nw2v_model.build_vocab(sentences, progress_per=10000)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:45:52.866569Z","iopub.execute_input":"2022-09-02T19:45:52.866992Z","iopub.status.idle":"2022-09-02T19:45:56.672605Z","shell.execute_reply.started":"2022-09-02T19:45:52.866952Z","shell.execute_reply":"2022-09-02T19:45:56.671585Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:46:07.880858Z","iopub.execute_input":"2022-09-02T19:46:07.881982Z","iopub.status.idle":"2022-09-02T19:48:32.952730Z","shell.execute_reply.started":"2022-09-02T19:46:07.881938Z","shell.execute_reply":"2022-09-02T19:48:32.951481Z"},"trusted":true},"execution_count":11,"outputs":[]}]}