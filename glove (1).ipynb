{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom gensim.models.fasttext import FastText\nfrom gensim.models.doc2vec import TaggedDocument, Doc2Vec\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import neighbors\nimport pickle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport gc\nimport nltk\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n\n\nfrom nltk.corpus import stopwords\nimport multiprocessing\n\ncores = multiprocessing.cpu_count()\nFastTextModel = \"fasttext_model.sav\"\n\nallContent = \"\"\npath1 = \"../input/ps-dataset/powershell_benign_dataset/\"\npath2 = \"../input/ps-dataset/malicious_pure\"\n#path3 = r\"D:\\Test Scripts\"\nlabels = []\ntext = []\ntestText = []\n#new_str = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', string)\nfName= []\nnewText = []\nregex = r':\\s*\\n+'\nvectorizer = CountVectorizer(stop_words='english', max_features=5000)\n\ndef test_bowser(path):\n        with open(path, encoding='utf-8') as f:\n            t_content = f.read()\n            t_content = t_content.lower()\n            t_content = t_content.replace(\"(\", \"\")\n            t_content = t_content.replace(\")\", \"\")\n            t_content = t_content.replace(\"[\", \"\")\n            t_content = t_content.replace(\"]\", \"\")\n            t_content = t_content.replace(\"$\", \"\")\n            t_content = t_content.replace(\"-\", \"\")\n            t_content = t_content.replace(\"=\", \"\")\n            t_content = t_content.replace(\"\", \"\")\n            t_content = t_content.replace(\"{\", \"\")\n            t_content = t_content.replace(\"}\", \"\")\n            t_content = t_content.replace(\".\", \"\")\n            t_content = t_content.replace(\"_\", \"\")\n            t_content = t_content.replace(\"|\", \"\")\n            t_content = t_content.replace(\":\", \"\")\n            t_content = t_content.replace(\"|\", \"\")\n            t_content = t_content.replace(\"/\", \"\")\n            t_content = t_content.replace(\"@\", \"\")\n            t_content = t_content.replace(\"?\", \"\")\n            t_content = t_content.replace(\",\", \"\")\n            t_content = t_content.replace(\">\", \"\")\n            t_content = t_content.replace(\"<\", \"\")\n            t_content = t_content.replace(\";\", \"\")\n            t_content = t_content.replace(\"\\\\\", \"\")\n            t_content = t_content.replace(\"+\", \"\")\n            t_content = t_content.replace(\"\\\"\",\"\")\n            t_content = t_content.replace(\"\\'\",\"\")\n            t_content = t_content.replace(\"`\", \"\")\n            t_content = t_content.replace(\"*\", \"\")\n            t_content = \" \".join(t_content.split())\n            newText.append(t_content)\n            return t_content\n\ndef bowser(path, label):\n    for filename in os.listdir(path):\n        labelName = \"__label__\" + label\n        labels.append(labelName)\n        filename = os.path.join(path, filename)\n        with open(filename, encoding='utf-8') as f:\n                    content = f.read()\n                    content = content.lower()\n                    content = content.replace(\"(\", \"\")\n                    content = content.replace(\")\", \"\")\n                    content = content.replace(\"[\", \"\")\n                    content = content.replace(\"]\", \"\")\n                    content = content.replace(\"$\", \"\")\n                    content = content.replace(\"-\", \"\")\n                    content = content.replace(\"!\",\"\")\n                    content = content.replace(\"=\", \"\")\n                    content = content.replace(\"\", \"\")\n                    content = content.replace(\"{\", \"\")\n                    content = content.replace(\"}\", \"\")\n                    content = content.replace(\".\", \"\")\n                    content = content.replace(\"_\", \"\")\n                    content = content.replace(\"|\", \"\")\n                    content = content.replace(\":\", \"\")\n                    content = content.replace(\"|\", \"\")\n                    content = content.replace(\"/\", \"\")\n                    content = content.replace(\"@\", \"\")\n                    content = content.replace(\"?\", \"\")\n                    content = content.replace(\",\", \"\")\n                    content = content.replace(\">\", \"\")\n                    content = content.replace(\"<\", \"\")\n                    content = content.replace(\";\", \"\")\n                    content = content.replace(\"\\\\\", \"\")\n                    content = content.replace(\"+\", \"\")\n                    content = content.replace(\"\\\"\", \"\")\n                    content = content.replace(\"\\'\", \"\")\n                    content = content.replace(\"`\", \"\")\n                    content = content.replace(\"*\", \"\")\n                    content = \" \".join(content.split())\n                    testText.append(content)\n\n\n#test_bowser(path3)\nbowser(path1,\"Benign\")\nbowser(path2,\"Malicious\")\n#print(testText[6])\nnumFiles = len(testText)\n#res = [re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", ele) for ele in testText]\n#print(res[1] + \"\\n\")\n\ndf_main = pd.DataFrame(testText, columns=[\"Powershell Code\"])\ndf_main['Labels'] = labels\n#df.to_csv('tester.csv')\n\ntrain, test = train_test_split(df_main, test_size=0.3, random_state=42)\ntrain_labels = train['Labels']\ntrain_samples = train['Powershell Code']\ntest_labels = test['Labels']\ntest_samples = test['Powershell Code']\n#train_size = len(train['Powershell Code'])\ntrain.reset_index()\ntrain.reindex(index=range(0,5962))\n#print(train_samples)\n\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            if len(word) < 2:\n                continue\n            tokens.append(word.lower())\n    return tokens\n\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\n\nvectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\ntext_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\nvectorizer.adapt(text_ds)\n\nvectorizer.get_vocabulary()[:5]\n\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:48:15.061288Z","iopub.execute_input":"2022-09-02T19:48:15.061921Z","iopub.status.idle":"2022-09-02T19:48:17.327082Z","shell.execute_reply.started":"2022-09-02T19:48:15.061887Z","shell.execute_reply":"2022-09-02T19:48:17.326031Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path_to_glove_file = \"../input/glove6b/glove.6B.100d.txt\"\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:48:17.328302Z","iopub.execute_input":"2022-09-02T19:48:17.328532Z","iopub.status.idle":"2022-09-02T19:48:26.913628Z","shell.execute_reply.started":"2022-09-02T19:48:17.328503Z","shell.execute_reply":"2022-09-02T19:48:26.912252Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"voc = vectorizer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\nnum_tokens = len(voc) + 2\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:48:26.917732Z","iopub.execute_input":"2022-09-02T19:48:26.918064Z","iopub.status.idle":"2022-09-02T19:48:27.283299Z","shell.execute_reply.started":"2022-09-02T19:48:26.918035Z","shell.execute_reply":"2022-09-02T19:48:27.282192Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding\n\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n)\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:48:27.284518Z","iopub.execute_input":"2022-09-02T19:48:27.284825Z","iopub.status.idle":"2022-09-02T19:48:27.447931Z","shell.execute_reply.started":"2022-09-02T19:48:27.284795Z","shell.execute_reply":"2022-09-02T19:48:27.446890Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\n\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nx = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\nx = layers.MaxPooling1D(5)(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.MaxPooling1D(5)(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dense(128, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\npreds = layers.Dense(len(labels), activation=\"softmax\")(x)\nmodel = keras.Model(int_sequences_input, preds)\nmodel.summary()\n\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:48:27.449799Z","iopub.execute_input":"2022-09-02T19:48:27.450760Z","iopub.status.idle":"2022-09-02T19:48:27.711902Z","shell.execute_reply.started":"2022-09-02T19:48:27.450732Z","shell.execute_reply":"2022-09-02T19:48:27.710975Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(df):\n    df = reduce_mem_usage(df)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:48:27.713043Z","iopub.execute_input":"2022-09-02T19:48:27.713364Z","iopub.status.idle":"2022-09-02T19:48:27.726235Z","shell.execute_reply.started":"2022-09-02T19:48:27.713334Z","shell.execute_reply":"2022-09-02T19:48:27.725053Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n\n#x_train = vectorizer(train_samples)\n#y_train = vectorizer(train_labels)\n\n\nx_test = vectorizer(np.array([[s] for s in test_samples])).numpy()\ny_test = vectorizer(np.array([[s] for s in test_labels])).numpy()\n\nprint(len(x_train))\nprint(len(y_train))\n\nprint(len(x_test))\nprint(len(y_test))\n#x_val = vectorizer(np.array([[s] for s in test_samples])).numpy()\n\n#y_train = np.array(train_labels)\n#y_val = np.array(test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T19:48:27.727429Z","iopub.execute_input":"2022-09-02T19:48:27.727712Z","iopub.status.idle":"2022-09-02T19:48:27.998240Z","shell.execute_reply.started":"2022-09-02T19:48:27.727688Z","shell.execute_reply":"2022-09-02T19:48:27.996502Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}